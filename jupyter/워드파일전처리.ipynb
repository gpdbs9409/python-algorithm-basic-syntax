{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8305766f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstory.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# this is your drive\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Prompting the user to upload a file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Opening the file in read mode and reading its content\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "# Import regular expressions library and a utility from Google Colab for file uploads\n",
    "import re\n",
    "\n",
    "data_path = \"story.txt\"  # this is your drive\n",
    "# Prompting the user to upload a file\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Opening the file in read mode and reading its content\n",
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    story_text = file.read()\n",
    "\n",
    "\n",
    "# definition of text preprocess function\n",
    "def preprocess(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = text.strip()\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "\n",
    "# Imagine we have a function called preprocess that cleans up our text\n",
    "words = preprocess(story_text)\n",
    "\n",
    "def create_vocabulary(words):\n",
    "    unique_words = set(words)  # 중복을 제거하여 고유 단어 집합 생성\n",
    "    word_to_index = {word: i for i, word in enumerate(unique_words)}  # 단어에 고유한 인덱스 부여\n",
    "    index_to_word = {i: word for word, i in word_to_index.items()}  # 인덱스에서 단어로의 매핑 생성\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "\n",
    "# Now we're making a list of unique words and assigning them a unique identifier\n",
    "word_to_index, index_to_word = create_vocabulary(words)\n",
    "\n",
    "# Here's where we start preparing our data for a simple machine learning model. We don't use fancy libraries here.\n",
    "# This function makes pairs of words that are close to each other in the text.\n",
    "def generate_training_data(words, word_to_index, window_size=2):\n",
    "    X = []  # This will hold our \"input\" data\n",
    "    y = []  # This will hold our \"output\" data\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(max(i - window_size, 0), min(i + window_size + 1, len(words))):\n",
    "            if i != j:  # We don't want to pair a word with itself\n",
    "                X.append(word_to_index[word])\n",
    "                y.append(word_to_index[words[j]])\n",
    "    return X, y\n",
    "\n",
    "# Using our function to get our training data ready\n",
    "X, y = generate_training_data(words, word_to_index)\n",
    "\n",
    "# To compare words, we're going to use something called cosine similarity. It's a fancy way of saying \"how similar are these?\"\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    dot_product = np.dot(vec_a, vec_b)  # Multiplying the vectors\n",
    "    norm_a = np.linalg.norm(vec_a)  # Getting the magnitude of the first vector\n",
    "    norm_b = np.linalg.norm(vec_b)  # And the second\n",
    "    return dot_product / (norm_a * norm_b)  # Dividing the product by the magnitudes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Let's pretend we've already trained our model and got these vectors for two words\n",
    "vector_a = np.random.rand(100)  # Just a random vector for demonstration\n",
    "vector_b = np.random.rand(100)  # Another random vector\n",
    "\n",
    "# Calculating how similar our two pretend word vectors are\n",
    "similarity = cosine_similarity(vector_a, vector_b)\n",
    "\n",
    "print(similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
